{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Natural Language Processing to Generate New Recipes in Python\n",
    "## Task 4: GPT-2 Pretrained Model Formulation and Theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "\n",
    "We will be using `Distilled GPT2` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bccc98772c8f4b50b9fe064ec2586d5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=762.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebd881a9195243ab95ea1954fafdc35f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1042301.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b0668aca0b746eaa6e304fe4fdbefc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7ea07e9f0834fdc842fd23fcc10426e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=352833716.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# downloading the 2 models\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilgpt2')\n",
    "model = AutoModelForCausalLM.from_pretrained('distilgpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[28065,    11,   314,  6807,   284,   262,  6128,    11,   290]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "prompt_text = 'Yesterday, I walked to the shop, and'\n",
    "\n",
    "encoded_prompt = tokenizer.encode(\n",
    "    prompt_text,\n",
    "    add_special_tokens=False,\n",
    "    # if you want to use tensorflow use 'tf'\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "encoded_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    }
   ],
   "source": [
    "output_sequences = model.generate(\n",
    "    input_ids= encoded_prompt,\n",
    "    max_length=2048,\n",
    "    temprature= 0.9,\n",
    "    # potential possibilities\n",
    "    top_k=20,\n",
    "    # top probabilities\n",
    "    top_p=0.9,\n",
    "    # how much it should penalize repeating the same word in an input from1 to infinity\n",
    "    repetition_penalty= 1,\n",
    "    do_sample=True,\n",
    "    # how many sequences\n",
    "    num_return_sequences=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([28065,    11,   314,  ...,  1178,   286, 50256])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Yesterday, I walked to the shop, and I found a few shelves of the new book on the counter. There was a bunch of new titles I had just started reading, and I had no idea how much I'd read or read on the shelves of those books. It was like this: I was in my mid-60s, and my parents had bought a new, new, new, old paperback edition of the New York Times bestseller. I could not tell it was the kind of book I needed, so I was going to try and find out how much I had read on the shelves of those books and how many times I'd had a chance to try to read them, and how many times I'd had a chance to go to see the reviews. I was looking for one of these books that I couldn't find online. I went to check out the review and see if it was really the best book I had ever read. I had a couple of other books that I found online:\\nIn the past year, I read a book called The Complete Guide to Modern Literature. It's a very well-written book written by a number of writers, including Jonathan Greenblatt, who has written several books about his works and his works. The book takes you through a few of the topics and is a great resource for me.\\nThe first book I read in the book was The Complete Guide to Modern Literature. It's a very well-written book written by a number of writers, including Jonathan Greenblatt, who has written several books about his works and his works. The book takes you through a few of the topics and is a great resource for me. The book takes you through a few of the topics and is a great resource for me. The book takes you through a few of the topics and is a great resource for me. The book takes you through a few of the topics and is a great resource for me. The book is a lot of great work. The books are great.\\nThe last book I read, The Complete Guide to Modern Literature, is a collection of short stories by a number of writers, including Jonathan Greenblatt, who has written several books about his works and his works. The book takes you through a few of the topics and is a great resource for me. The book takes you through a few of the topics and is a great resource for me. The book takes you through a few of the topics and is a great resource for me. The book takes you through a few of the topics and is a great resource for me. The book is a lot of great work. The books are great.\\nThe last book I read, The Complete Guide to Modern Literature, is a collection of short stories by a number of writers, including Jonathan Greenblatt, who has written several books about his works and his works. The book takes you through a few of the topics and is a great resource for me. The book takes you through a few of the topics and is a great resource for me. The book is a lot of great work. The books are great.\\nThis book is really a great resource for me.\\nThe last book I read, The Complete Guide to Modern Literature, is a collection of short stories by a number of writers, including Jonathan Greenblatt, who has written several books about his works and his works. The book takes you through a few of the topics and is a great resource for me. The book is a lot of great work. The books are great.\\nThe last book I read, The Complete Guide to Modern Literature, is a collection of short stories by a number of writers, including Jonathan Greenblatt, who has written several books about his works and his works. The book takes you through a few of the topics and is a great resource for me. The book takes you through a few of the topics and is a great resource for me. The book takes you through a few of the topics and is a great resource for me. The book takes you through a few of the topics and is a great resource for me. The book takes you through a few of the topics and is a great resource for me. The book takes you through a few of the topics and is a great resource for me. The book takes you through a few of the topics and is a great resource for me. The book takes you through a few of the topics and is a great resource for me. The book takes you through a few of the topics and is a great resource for me. The book takes you through a few of the topics and is a great resource for me. The book takes you through a few of the topics and is a great resource for me. The book takes you through a few of the topics and is a great resource for me. The book takes you through a few of the topics and is a great resource for me. The book takes you through a few of the topics and is a great resource for me. The book takes you through a few of<|endoftext|>\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minput_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmin_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdo_sample\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mearly_stopping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnum_beams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtemperature\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtop_k\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtop_p\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrepetition_penalty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbad_words_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbos_token_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpad_token_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0meos_token_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlength_penalty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mno_repeat_ngram_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnum_return_sequences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mattention_mask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdecoder_start_token_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0muse_cache\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mmodel_specific_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Generates sequences for models with a LM head. The method currently supports greedy decoding, beam-search decoding, sampling with temperature, sampling with top-k or nucleus sampling.\n",
       "\n",
       "Adapted in part from `Facebook's XLM beam search code`_.\n",
       "\n",
       ".. _`Facebook's XLM beam search code`:\n",
       "   https://github.com/facebookresearch/XLM/blob/9e6f6814d17be4fe5b15f2e6c43eb2b2d76daeb4/src/model/transformer.py#L529\n",
       "\n",
       "\n",
       "Parameters:\n",
       "\n",
       "    input_ids: (`optional`) `torch.LongTensor` of shape `(batch_size, sequence_length)`\n",
       "        The sequence used as a prompt for the generation. If `None` the method initializes\n",
       "        it as an empty `torch.LongTensor` of shape `(1,)`.\n",
       "\n",
       "    max_length: (`optional`) int\n",
       "        The max length of the sequence to be generated.  Between `min_length` and infinity. Default to 20.\n",
       "\n",
       "    min_length: (`optional`) int\n",
       "        The min length of the sequence to be generated.  Between 0 and infinity. Default to 0.\n",
       "\n",
       "    do_sample: (`optional`) bool\n",
       "        If set to `False` greedy decoding is used. Otherwise sampling is used. Defaults to `False` as defined in `configuration_utils.PretrainedConfig`.\n",
       "\n",
       "    early_stopping: (`optional`) bool\n",
       "        if set to `True` beam search is stopped when at least `num_beams` sentences finished per batch. Defaults to `False` as defined in `configuration_utils.PretrainedConfig`.\n",
       "\n",
       "    num_beams: (`optional`) int\n",
       "        Number of beams for beam search. Must be between 1 and infinity. 1 means no beam search. Default to 1.\n",
       "\n",
       "    temperature: (`optional`) float\n",
       "        The value used to module the next token probabilities. Must be strictly positive. Default to 1.0.\n",
       "\n",
       "    top_k: (`optional`) int\n",
       "        The number of highest probability vocabulary tokens to keep for top-k-filtering. Between 1 and infinity. Default to 50.\n",
       "\n",
       "    top_p: (`optional`) float\n",
       "        The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Must be between 0 and 1. Default to 1.\n",
       "\n",
       "    repetition_penalty: (`optional`) float\n",
       "        The parameter for repetition penalty. Between 1.0 and infinity. 1.0 means no penalty. Default to 1.0.\n",
       "\n",
       "    pad_token_id: (`optional`) int\n",
       "        Padding token. Default to specicic model pad_token_id or None if it does not exist.\n",
       "\n",
       "    bos_token_id: (`optional`) int\n",
       "        BOS token. Defaults to `bos_token_id` as defined in the models config.\n",
       "\n",
       "    eos_token_id: (`optional`) int\n",
       "        EOS token. Defaults to `eos_token_id` as defined in the models config.\n",
       "\n",
       "    length_penalty: (`optional`) float\n",
       "        Exponential penalty to the length. Default to 1.\n",
       "\n",
       "    no_repeat_ngram_size: (`optional`) int\n",
       "        If set to int > 0, all ngrams of size `no_repeat_ngram_size` can only occur once.\n",
       "    bad_words_ids: (`optional`) list of lists of int\n",
       "        `bad_words_ids` contains tokens that are not allowed to be generated. In order to get the tokens of the words that should not appear in the generated text, use `tokenizer.encode(bad_word, add_prefix_space=True)`.\n",
       "\n",
       "    num_return_sequences: (`optional`) int\n",
       "        The number of independently computed returned sequences for each element in the batch. Default to 1.\n",
       "\n",
       "    attention_mask (`optional`) obj: `torch.LongTensor` of same shape as `input_ids`\n",
       "        Mask to avoid performing attention on padding token indices.\n",
       "        Mask values selected in ``[0, 1]``:\n",
       "        ``1`` for tokens that are NOT MASKED, ``0`` for MASKED tokens.\n",
       "        Defaults to `None`.\n",
       "\n",
       "        `What are attention masks? <../glossary.html#attention-mask>`__\n",
       "\n",
       "    decoder_start_token_id=None: (`optional`) int\n",
       "        If an encoder-decoder model starts decoding with a different token than BOS.\n",
       "        Defaults to `None` and is changed to `BOS` later.\n",
       "\n",
       "    use_cache: (`optional`) bool\n",
       "        If `use_cache` is True, past key values are used to speed up decoding if applicable to model. Defaults to `True`.\n",
       "\n",
       "    model_specific_kwargs: (`optional`) dict\n",
       "        Additional model specific kwargs will be forwarded to the `forward` function of the model.\n",
       "\n",
       "Return:\n",
       "\n",
       "    output: `torch.LongTensor` of shape `(batch_size * num_return_sequences, sequence_length)`\n",
       "        sequence_length is either equal to max_length or shorter if all batches finished early due to the `eos_token_id`\n",
       "\n",
       "Examples::\n",
       "\n",
       "    tokenizer = AutoTokenizer.from_pretrained('distilgpt2')   # Initialize tokenizer\n",
       "    model = AutoModelWithLMHead.from_pretrained('distilgpt2')    # Download model and configuration from S3 and cache.\n",
       "    outputs = model.generate(max_length=40)  # do greedy decoding\n",
       "    print('Generated: {}'.format(tokenizer.decode(outputs[0], skip_special_tokens=True)))\n",
       "\n",
       "    tokenizer = AutoTokenizer.from_pretrained('openai-gpt')   # Initialize tokenizer\n",
       "    model = AutoModelWithLMHead.from_pretrained('openai-gpt')    # Download model and configuration from S3 and cache.\n",
       "    input_context = 'The dog'\n",
       "    input_ids = tokenizer.encode(input_context, return_tensors='pt')  # encode input context\n",
       "    outputs = model.generate(input_ids=input_ids, num_beams=5, num_return_sequences=3, temperature=1.5)  # generate 3 independent sequences using beam search decoding (5 beams) with sampling from initial context 'The dog'\n",
       "    for i in range(3): #  3 output sequences were generated\n",
       "        print('Generated {}: {}'.format(i, tokenizer.decode(outputs[i], skip_special_tokens=True)))\n",
       "\n",
       "    tokenizer = AutoTokenizer.from_pretrained('distilgpt2')   # Initialize tokenizer\n",
       "    model = AutoModelWithLMHead.from_pretrained('distilgpt2')    # Download model and configuration from S3 and cache.\n",
       "    input_context = 'The dog'\n",
       "    input_ids = tokenizer.encode(input_context, return_tensors='pt')  # encode input context\n",
       "    outputs = model.generate(input_ids=input_ids, max_length=40, temperature=0.7, num_return_sequences=3)  # 3 generate sequences using by sampling\n",
       "    for i in range(3): #  3 output sequences were generated\n",
       "        print('Generated {}: {}'.format(i, tokenizer.decode(outputs[i], skip_special_tokens=True)))\n",
       "\n",
       "    tokenizer = AutoTokenizer.from_pretrained('ctrl')   # Initialize tokenizer\n",
       "    model = AutoModelWithLMHead.from_pretrained('ctrl')    # Download model and configuration from S3 and cache.\n",
       "    input_context = 'Legal My neighbor is'  # \"Legal\" is one of the control codes for ctrl\n",
       "    input_ids = tokenizer.encode(input_context, return_tensors='pt')  # encode input context\n",
       "    outputs = model.generate(input_ids=input_ids, max_length=50, temperature=0.7, repetition_penalty=1.2)  # generate sequences\n",
       "    print('Generated: {}'.format(tokenizer.decode(outputs[0], skip_special_tokens=True)))\n",
       "\n",
       "    tokenizer = AutoTokenizer.from_pretrained('gpt2')   # Initialize tokenizer\n",
       "    model = AutoModelWithLMHead.from_pretrained('gpt2')    # Download model and configuration from S3 and cache.\n",
       "    input_context = 'My cute dog'  # \"Legal\" is one of the control codes for ctrl\n",
       "    bad_words_ids = [tokenizer.encode(bad_word, add_prefix_space=True) for bad_word in ['idiot', 'stupid', 'shut up']]\n",
       "    input_ids = tokenizer.encode(input_context, return_tensors='pt')  # encode input context\n",
       "    outputs = model.generate(input_ids=input_ids, max_length=100, do_sample=True, bad_words_ids=bad_words_ids)  # generate sequences without allowing bad_words to be generated\n",
       "\u001b[0;31mFile:\u001b[0m      ~/.local/lib/python3.6/site-packages/transformers/generation_utils.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.generate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
